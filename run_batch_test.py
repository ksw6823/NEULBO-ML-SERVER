#!/usr/bin/env python3
"""
NEULBO ML Server ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ëŸ¬ë„ˆ

ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
"""

import json
import requests
import time
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
import statistics

class BatchTester:
    """ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self, server_url: str = "http://localhost:8000"):
        self.server_url = server_url
        self.results = []
        
    def check_server_health(self) -> bool:
        """ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            response = requests.get(f"{self.server_url}/api/v1/health/check", timeout=10)
            if response.status_code == 200:
                health = response.json()
                print(f"âœ… ì„œë²„ ìƒíƒœ: {health['status']}")
                print(f"   ë°ì´í„°ë² ì´ìŠ¤: {health['database_status']}")
                print(f"   ëª¨ë¸: {health['model_status']}")
                return health['status'] == 'healthy'
            return False
        except Exception as e:
            print(f"âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            return False
    
    def run_single_test(self, test_file: Path) -> Dict[str, Any]:
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        try:
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©
            with open(test_file, 'r', encoding='utf-8') as f:
                test_data = json.load(f)
            
            print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰: {test_file.name}")
            print(f"   ğŸ“ {test_data.get('description', 'No description')}")
            print(f"   â±ï¸  ì‹œê°„: {test_data['metadata']['duration_hours']:.1f}ì‹œê°„")
            print(f"   ğŸ“Š ë°ì´í„°: {test_data['metadata']['data_points']}ê°œ í¬ì¸íŠ¸")
            
            # API í˜¸ì¶œ
            start_time = time.time()
            response = requests.post(
                f"{self.server_url}/api/v1/sleep/analyze",
                json={k: v for k, v in test_data.items() 
                      if k not in ['description', 'expected_stages', 'metadata']},
                timeout=120  # 2ë¶„ íƒ€ì„ì•„ì›ƒ
            )
            end_time = time.time()
            
            processing_time = end_time - start_time
            
            # ê²°ê³¼ ì²˜ë¦¬
            if response.status_code == 200:
                result = response.json()
                
                test_result = {
                    'test_file': test_file.name,
                    'user_id': test_data['user_id'],
                    'description': test_data.get('description', ''),
                    'status': 'success',
                    'processing_time': round(processing_time, 2),
                    'analysis_id': result['analysis_id'],
                    'data_quality_score': result['data_quality_score'],
                    'total_sleep_time': result['summary_statistics']['total_sleep_time'],
                    'sleep_efficiency': result['summary_statistics']['sleep_efficiency'],
                    'wake_time': result['summary_statistics']['wake_time'],
                    'n1_time': result['summary_statistics']['n1_time'],
                    'n2_time': result['summary_statistics']['n2_time'],
                    'n3_time': result['summary_statistics']['n3_time'],
                    'rem_time': result['summary_statistics']['rem_time'],
                    'wake_percentage': result['summary_statistics']['wake_percentage'],
                    'n1_percentage': result['summary_statistics']['n1_percentage'],
                    'n2_percentage': result['summary_statistics']['n2_percentage'],
                    'n3_percentage': result['summary_statistics']['n3_percentage'],
                    'rem_percentage': result['summary_statistics']['rem_percentage'],
                    'model_version': result['model_version'],
                    'input_duration_hours': test_data['metadata']['duration_hours'],
                    'input_data_points': test_data['metadata']['data_points'],
                    'noise_level': test_data['metadata']['noise_level'],
                    'movement_level': test_data['metadata']['movement_level']
                }
                
                print(f"   âœ… ì„±ê³µ ({processing_time:.1f}ì´ˆ)")
                print(f"      ğŸ¯ í’ˆì§ˆì ìˆ˜: {result['data_quality_score']:.3f}")
                print(f"      ğŸ’¤ ì´ìˆ˜ë©´: {result['summary_statistics']['total_sleep_time']}ë¶„")
                print(f"      ğŸ“ˆ íš¨ìœ¨ì„±: {result['summary_statistics']['sleep_efficiency']:.1%}")
                
            else:
                test_result = {
                    'test_file': test_file.name,
                    'user_id': test_data['user_id'],
                    'description': test_data.get('description', ''),
                    'status': 'failed',
                    'processing_time': round(processing_time, 2),
                    'error_code': response.status_code,
                    'error_message': response.text[:200],
                    'input_duration_hours': test_data['metadata']['duration_hours'],
                    'input_data_points': test_data['metadata']['data_points']
                }
                
                print(f"   âŒ ì‹¤íŒ¨ ({response.status_code})")
                print(f"      ì˜¤ë¥˜: {response.text[:100]}...")
            
            return test_result
            
        except Exception as e:
            print(f"   ğŸ’¥ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
            return {
                'test_file': test_file.name,
                'status': 'error',
                'error_message': str(e)
            }
    
    def run_batch_tests(self, dataset_dir: Path) -> List[Dict[str, Any]]:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        
        print(f"ğŸš€ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print(f"ğŸ“ ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬: {dataset_dir}")
        print("=" * 60)
        
        # í…ŒìŠ¤íŠ¸ íŒŒì¼ ëª©ë¡
        test_files = list(dataset_dir.glob("*.json"))
        test_files = [f for f in test_files if f.name != "dataset_summary.json"]
        
        if not test_files:
            print("âŒ í…ŒìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"ğŸ“Š ì´ {len(test_files)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë°œê²¬")
        print()
        
        results = []
        
        for i, test_file in enumerate(test_files, 1):
            print(f"[{i}/{len(test_files)}] ", end="")
            result = self.run_single_test(test_file)
            results.append(result)
            print()
            
            # í…ŒìŠ¤íŠ¸ ê°„ ì ì‹œ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            if i < len(test_files):
                time.sleep(1)
        
        return results
    
    def generate_report(self, results: List[Dict[str, Any]]) -> None:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        print("=" * 60)
        print("ğŸ“Š ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬í¬íŠ¸")
        print("=" * 60)
        
        # ê¸°ë³¸ í†µê³„
        total_tests = len(results)
        successful_tests = len([r for r in results if r.get('status') == 'success'])
        failed_tests = total_tests - successful_tests
        
        print(f"ğŸ“ˆ ì „ì²´ ê²°ê³¼:")
        print(f"   ì´ í…ŒìŠ¤íŠ¸: {total_tests}ê°œ")
        print(f"   ì„±ê³µ: {successful_tests}ê°œ ({successful_tests/total_tests*100:.1f}%)")
        print(f"   ì‹¤íŒ¨: {failed_tests}ê°œ ({failed_tests/total_tests*100:.1f}%)")
        print()
        
        # ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ë“¤ì˜ ì„±ëŠ¥ ë¶„ì„
        successful_results = [r for r in results if r.get('status') == 'success']
        
        if successful_results:
            print("âš¡ ì„±ëŠ¥ ë¶„ì„:")
            
            processing_times = [r['processing_time'] for r in successful_results]
            quality_scores = [r['data_quality_score'] for r in successful_results]
            sleep_efficiencies = [r['sleep_efficiency'] for r in successful_results]
            
            print(f"   ì²˜ë¦¬ ì‹œê°„: í‰ê·  {statistics.mean(processing_times):.1f}ì´ˆ")
            print(f"            ìµœì†Œ {min(processing_times):.1f}ì´ˆ, ìµœëŒ€ {max(processing_times):.1f}ì´ˆ")
            print(f"   í’ˆì§ˆ ì ìˆ˜: í‰ê·  {statistics.mean(quality_scores):.3f}")
            print(f"            ìµœì†Œ {min(quality_scores):.3f}, ìµœëŒ€ {max(quality_scores):.3f}")
            print(f"   ìˆ˜ë©´ íš¨ìœ¨: í‰ê·  {statistics.mean(sleep_efficiencies):.1%}")
            print(f"            ìµœì†Œ {min(sleep_efficiencies):.1%}, ìµœëŒ€ {max(sleep_efficiencies):.1%}")
            print()
        
        # ìˆ˜ë©´ ë‹¨ê³„ ë¶„í¬ ë¶„ì„
        if successful_results:
            print("ğŸ’¤ ìˆ˜ë©´ ë‹¨ê³„ ë¶„ì„:")
            
            for stage in ['wake', 'n1', 'n2', 'n3', 'rem']:
                percentages = [r[f'{stage}_percentage'] for r in successful_results]
                avg_percentage = statistics.mean(percentages)
                print(f"   {stage.upper():4s}: í‰ê·  {avg_percentage:5.1f}%")
            print()
        
        # ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼
        print("ğŸ“‹ ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"{'íŒŒì¼ëª…':<25} {'ìƒíƒœ':<8} {'ì‹œê°„':<6} {'í’ˆì§ˆ':<6} {'íš¨ìœ¨':<6} {'ì„¤ëª…'}")
        print("-" * 80)
        
        for result in results:
            filename = result['test_file'][:24]
            status = "âœ… ì„±ê³µ" if result.get('status') == 'success' else "âŒ ì‹¤íŒ¨"
            
            if result.get('status') == 'success':
                time_str = f"{result['processing_time']:.1f}s"
                quality_str = f"{result['data_quality_score']:.3f}"
                efficiency_str = f"{result['sleep_efficiency']:.1%}"
            else:
                time_str = quality_str = efficiency_str = "-"
            
            description = result.get('description', '')[:30]
            
            print(f"{filename:<25} {status:<8} {time_str:<6} {quality_str:<6} {efficiency_str:<6} {description}")
        
        # CSV íŒŒì¼ ì €ì¥
        self.save_results_csv(results)
        
        print()
        print("ğŸ’¾ ê²°ê³¼ê°€ batch_test_results.csvì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def save_results_csv(self, results: List[Dict[str, Any]]) -> None:
        """ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        
        df = pd.DataFrame(results)
        
        # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
        column_order = [
            'test_file', 'user_id', 'description', 'status', 'processing_time',
            'data_quality_score', 'total_sleep_time', 'sleep_efficiency',
            'wake_percentage', 'n1_percentage', 'n2_percentage', 'n3_percentage', 'rem_percentage',
            'input_duration_hours', 'input_data_points', 'noise_level', 'movement_level',
            'analysis_id', 'model_version', 'error_code', 'error_message'
        ]
        
        # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
        available_columns = [col for col in column_order if col in df.columns]
        df = df[available_columns]
        
        # CSV ì €ì¥
        csv_filename = f"batch_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
        
        print(f"ğŸ“Š ìƒì„¸ ê²°ê³¼ê°€ {csv_filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("ğŸ§ª NEULBO ML Server ë°°ì¹˜ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ í™•ì¸
    dataset_dir = Path("test_dataset")
    
    if not dataset_dir.exists():
        print("âŒ test_dataset ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("   ë¨¼ì € í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ì„¸ìš”:")
        print("   python generate_test_dataset.py")
        return
    
    # ë°°ì¹˜ í…ŒìŠ¤í„° ì´ˆê¸°í™”
    tester = BatchTester()
    
    # ì„œë²„ ìƒíƒœ í™•ì¸
    if not tester.check_server_health():
        print("âŒ ì„œë²„ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ì„œë²„ë¥¼ ë¨¼ì € ì‹œì‘í•˜ì„¸ìš”: python run_server.py")
        return
    
    print()
    
    # ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = tester.run_batch_tests(dataset_dir)
    
    if results:
        # ë¦¬í¬íŠ¸ ìƒì„±
        tester.generate_report(results)
    else:
        print("âŒ ì‹¤í–‰í•  í…ŒìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    print("\nğŸ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
