#!/usr/bin/env python3
"""
NEULBO ML Server ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

ë‹¤ì–‘í•œ ë°ì´í„° í¬ê¸°ì™€ ì¡°ê±´ì—ì„œ ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
"""

import json
import requests
import time
import threading
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any

class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self, server_url: str = "http://localhost:8000"):
        self.server_url = server_url
        self.results = []
    
    def generate_benchmark_data(self, duration_hours: float, user_id: str) -> dict:
        """ë²¤ì¹˜ë§ˆí¬ìš© ë°ì´í„° ìƒì„±"""
        
        start_time = datetime(2024, 1, 15, 22, 0, 0)
        end_time = start_time + timedelta(hours=duration_hours)
        
        # 30ì´ˆ ê°„ê²© ë°ì´í„° í¬ì¸íŠ¸
        data_points = int(duration_hours * 120)  # ì‹œê°„ë‹¹ 120ê°œ í¬ì¸íŠ¸
        
        accelerometer_data = []
        audio_data = []
        
        for i in range(data_points):
            current_time = start_time + timedelta(seconds=i * 30)
            
            # ê°€ë²¼ìš´ ëœë¤ ë°ì´í„° (ë¹ ë¥¸ ìƒì„±)
            accelerometer_data.append({
                "timestamp": current_time.isoformat(),
                "x": np.random.normal(0.05, 0.1),
                "y": np.random.normal(-0.1, 0.1),
                "z": np.random.normal(9.8, 0.1)
            })
            
            audio_data.append({
                "timestamp": current_time.isoformat(),
                "amplitude": np.random.exponential(0.05),
                "frequency_bands": np.random.exponential(0.05, 8).tolist()
            })
        
        return {
            "user_id": user_id,
            "recording_start": start_time.isoformat(),
            "recording_end": end_time.isoformat(),
            "accelerometer_data": accelerometer_data,
            "audio_data": audio_data
        }
    
    def single_request_test(self, duration_hours: float, test_id: str) -> Dict[str, Any]:
        """ë‹¨ì¼ ìš”ì²­ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        
        print(f"ğŸ“Š {test_id}: {duration_hours}ì‹œê°„ ë°ì´í„° í…ŒìŠ¤íŠ¸")
        
        # ë°ì´í„° ìƒì„± ì‹œê°„ ì¸¡ì •
        data_gen_start = time.time()
        test_data = self.generate_benchmark_data(duration_hours, f"bench_{test_id}")
        data_gen_time = time.time() - data_gen_start
        
        data_size_mb = len(json.dumps(test_data).encode('utf-8')) / (1024 * 1024)
        
        print(f"   ğŸ“ ë°ì´í„° í¬ê¸°: {data_size_mb:.2f} MB")
        print(f"   ğŸ“ˆ ë°ì´í„° í¬ì¸íŠ¸: {len(test_data['accelerometer_data'])}ê°œ")
        print(f"   â±ï¸  ë°ì´í„° ìƒì„±: {data_gen_time:.2f}ì´ˆ")
        
        # API ìš”ì²­ ì„±ëŠ¥ ì¸¡ì •
        try:
            request_start = time.time()
            response = requests.post(
                f"{self.server_url}/api/v1/sleep/analyze",
                json=test_data,
                timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
            )
            request_time = time.time() - request_start
            
            if response.status_code == 200:
                result = response.json()
                
                benchmark_result = {
                    'test_id': test_id,
                    'duration_hours': duration_hours,
                    'data_points': len(test_data['accelerometer_data']),
                    'data_size_mb': data_size_mb,
                    'data_gen_time': data_gen_time,
                    'request_time': request_time,
                    'total_time': data_gen_time + request_time,
                    'throughput_points_per_sec': len(test_data['accelerometer_data']) / request_time,
                    'throughput_mb_per_sec': data_size_mb / request_time,
                    'status': 'success',
                    'data_quality_score': result['data_quality_score'],
                    'analysis_id': result['analysis_id']
                }
                
                print(f"   âœ… ì„±ê³µ: {request_time:.2f}ì´ˆ")
                print(f"   ğŸš€ ì²˜ë¦¬ëŸ‰: {benchmark_result['throughput_points_per_sec']:.1f} points/sec")
                print(f"   ğŸ’¾ ì²˜ë¦¬ëŸ‰: {benchmark_result['throughput_mb_per_sec']:.2f} MB/sec")
                
            else:
                benchmark_result = {
                    'test_id': test_id,
                    'duration_hours': duration_hours,
                    'data_points': len(test_data['accelerometer_data']),
                    'data_size_mb': data_size_mb,
                    'request_time': request_time,
                    'status': 'failed',
                    'error_code': response.status_code,
                    'error_message': response.text[:100]
                }
                
                print(f"   âŒ ì‹¤íŒ¨: {response.status_code}")
        
        except Exception as e:
            benchmark_result = {
                'test_id': test_id,
                'duration_hours': duration_hours,
                'status': 'error',
                'error_message': str(e)
            }
            print(f"   ğŸ’¥ ì˜¤ë¥˜: {str(e)}")
        
        return benchmark_result
    
    def data_size_scaling_test(self) -> List[Dict[str, Any]]:
        """ë°ì´í„° í¬ê¸°ë³„ í™•ì¥ì„± í…ŒìŠ¤íŠ¸"""
        
        print("ğŸ“ˆ ë°ì´í„° í¬ê¸°ë³„ í™•ì¥ì„± í…ŒìŠ¤íŠ¸")
        print("=" * 50)
        
        # ë‹¤ì–‘í•œ ë°ì´í„° í¬ê¸° í…ŒìŠ¤íŠ¸
        test_durations = [1, 2, 4, 6, 8, 10, 12]  # ì‹œê°„
        results = []
        
        for i, duration in enumerate(test_durations):
            print(f"\n[{i+1}/{len(test_durations)}] ", end="")
            result = self.single_request_test(duration, f"scale_{i+1}")
            results.append(result)
        
        return results
    
    def concurrent_load_test(self, num_concurrent: int = 5, duration_hours: float = 4) -> List[Dict[str, Any]]:
        """ë™ì‹œ ìš”ì²­ ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
        
        print(f"\nğŸ”„ ë™ì‹œ ìš”ì²­ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ({num_concurrent}ê°œ ë™ì‹œ ìš”ì²­)")
        print("=" * 50)
        
        def single_concurrent_request(request_id: int) -> Dict[str, Any]:
            return self.single_request_test(duration_hours, f"concurrent_{request_id}")
        
        # ë™ì‹œ ìš”ì²­ ì‹¤í–‰
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=num_concurrent) as executor:
            futures = [
                executor.submit(single_concurrent_request, i) 
                for i in range(num_concurrent)
            ]
            
            results = []
            for i, future in enumerate(as_completed(futures)):
                result = future.result()
                results.append(result)
                print(f"ì™„ë£Œ: {i+1}/{num_concurrent}")
        
        total_time = time.time() - start_time
        
        print(f"\nğŸ“Š ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"   ì´ ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"   ì„±ê³µ ìš”ì²­: {len([r for r in results if r.get('status') == 'success'])}ê°œ")
        print(f"   ì‹¤íŒ¨ ìš”ì²­: {len([r for r in results if r.get('status') != 'success'])}ê°œ")
        
        return results
    
    def memory_stress_test(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ (ë§¤ìš° í° ë°ì´í„°)"""
        
        print("\nğŸ’¾ ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ (24ì‹œê°„ ë°ì´í„°)")
        print("=" * 50)
        
        # 24ì‹œê°„ ë°ì´í„° (ë§¤ìš° í° ë°ì´í„°ì…‹)
        return self.single_request_test(24, "memory_stress")
    
    def generate_performance_report(self, scaling_results: List[Dict], concurrent_results: List[Dict], stress_result: Dict):
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        print("\n" + "=" * 60)
        print("ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸")
        print("=" * 60)
        
        # í™•ì¥ì„± í…ŒìŠ¤íŠ¸ ë¶„ì„
        successful_scaling = [r for r in scaling_results if r.get('status') == 'success']
        
        if successful_scaling:
            print("\nğŸ“ˆ ë°ì´í„° í¬ê¸°ë³„ í™•ì¥ì„±:")
            print(f"{'ì‹œê°„':<6} {'í¬ì¸íŠ¸':<8} {'í¬ê¸°(MB)':<10} {'ì²˜ë¦¬ì‹œê°„':<8} {'ì²˜ë¦¬ëŸ‰(pts/s)':<12}")
            print("-" * 50)
            
            for result in successful_scaling:
                print(f"{result['duration_hours']:<6.1f} "
                      f"{result['data_points']:<8} "
                      f"{result['data_size_mb']:<10.2f} "
                      f"{result['request_time']:<8.2f} "
                      f"{result['throughput_points_per_sec']:<12.1f}")
            
            # ì„ í˜•ì„± ë¶„ì„
            durations = [r['duration_hours'] for r in successful_scaling]
            times = [r['request_time'] for r in successful_scaling]
            
            if len(durations) > 1:
                correlation = np.corrcoef(durations, times)[0, 1]
                print(f"\n   ğŸ“Š ì„ í˜•ì„± (ìƒê´€ê³„ìˆ˜): {correlation:.3f}")
                
                if correlation > 0.9:
                    print("   âœ… ì¢‹ì€ ì„ í˜• í™•ì¥ì„±")
                elif correlation > 0.7:
                    print("   âš ï¸  ì ë‹¹í•œ í™•ì¥ì„±")
                else:
                    print("   âŒ ë¹„ì„ í˜• í™•ì¥ì„± - ìµœì í™” í•„ìš”")
        
        # ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸ ë¶„ì„
        successful_concurrent = [r for r in concurrent_results if r.get('status') == 'success']
        
        if successful_concurrent:
            avg_time = np.mean([r['request_time'] for r in successful_concurrent])
            min_time = min([r['request_time'] for r in successful_concurrent])
            max_time = max([r['request_time'] for r in successful_concurrent])
            
            print(f"\nğŸ”„ ë™ì‹œ ìš”ì²­ ì„±ëŠ¥:")
            print(f"   í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time:.2f}ì´ˆ")
            print(f"   ìµœì†Œ ì²˜ë¦¬ ì‹œê°„: {min_time:.2f}ì´ˆ")
            print(f"   ìµœëŒ€ ì²˜ë¦¬ ì‹œê°„: {max_time:.2f}ì´ˆ")
            print(f"   ì‹œê°„ í¸ì°¨: {max_time - min_time:.2f}ì´ˆ")
        
        # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ê²°ê³¼
        if stress_result.get('status') == 'success':
            print(f"\nğŸ’¾ ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸:")
            print(f"   24ì‹œê°„ ë°ì´í„° ì²˜ë¦¬: âœ… ì„±ê³µ")
            print(f"   ì²˜ë¦¬ ì‹œê°„: {stress_result['request_time']:.2f}ì´ˆ")
            print(f"   ë°ì´í„° í¬ê¸°: {stress_result['data_size_mb']:.2f} MB")
            print(f"   ì²˜ë¦¬ëŸ‰: {stress_result['throughput_points_per_sec']:.1f} points/sec")
        else:
            print(f"\nğŸ’¾ ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸: âŒ ì‹¤íŒ¨")
        
        # ì„±ëŠ¥ ì§€í‘œ ìš”ì•½
        all_successful = successful_scaling + successful_concurrent
        if stress_result.get('status') == 'success':
            all_successful.append(stress_result)
        
        if all_successful:
            all_times = [r['request_time'] for r in all_successful]
            all_throughputs = [r['throughput_points_per_sec'] for r in all_successful]
            
            print(f"\nğŸ¯ ì „ì²´ ì„±ëŠ¥ ìš”ì•½:")
            print(f"   í‰ê·  ì²˜ë¦¬ ì‹œê°„: {np.mean(all_times):.2f}ì´ˆ")
            print(f"   ìµœê³  ì²˜ë¦¬ëŸ‰: {max(all_throughputs):.1f} points/sec")
            print(f"   ìµœì € ì²˜ë¦¬ëŸ‰: {min(all_throughputs):.1f} points/sec")
        
        # CSV ì €ì¥
        self.save_benchmark_csv(scaling_results + concurrent_results + [stress_result])
    
    def save_benchmark_csv(self, results: List[Dict]):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥"""
        
        df = pd.DataFrame(results)
        csv_filename = f"benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
        
        print(f"\nğŸ’¾ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ê°€ {csv_filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("âš¡ NEULBO ML Server ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    print("=" * 50)
    
    # ì„œë²„ ìƒíƒœ í™•ì¸
    try:
        response = requests.get("http://localhost:8000/api/v1/health/check", timeout=10)
        if response.status_code != 200:
            print("âŒ ì„œë²„ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
    except:
        print("âŒ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   ì„œë²„ë¥¼ ë¨¼ì € ì‹œì‘í•˜ì„¸ìš”: python run_server.py")
        return
    
    benchmark = PerformanceBenchmark()
    
    print("ğŸš€ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("âš ï¸  ì£¼ì˜: ì´ í…ŒìŠ¤íŠ¸ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # 1. ë°ì´í„° í¬ê¸°ë³„ í™•ì¥ì„± í…ŒìŠ¤íŠ¸
    scaling_results = benchmark.data_size_scaling_test()
    
    # 2. ë™ì‹œ ìš”ì²­ ë¶€í•˜ í…ŒìŠ¤íŠ¸
    concurrent_results = benchmark.concurrent_load_test(num_concurrent=3, duration_hours=2)
    
    # 3. ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ (ì„ íƒì )
    print("\nâ“ ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (24ì‹œê°„ ë°ì´í„°, ë§¤ìš° ì˜¤ë˜ ê±¸ë¦¼)")
    run_stress = input("   y/N: ").lower().strip() == 'y'
    
    if run_stress:
        stress_result = benchmark.memory_stress_test()
    else:
        stress_result = {'status': 'skipped', 'test_id': 'memory_stress'}
    
    # 4. ë¦¬í¬íŠ¸ ìƒì„±
    benchmark.generate_performance_report(scaling_results, concurrent_results, stress_result)
    
    print("\nğŸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
